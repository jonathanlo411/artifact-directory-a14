Title:
Auditing Sentiment Analysis Algorithms for Bias

Abstract:
Sentiment analysis algorithms are steadily being integrated across the web to help with tasks such as content moderation. On many major websites, whether or not a comment is allowed to be posted at least partially depends on the output of a sentiment analysis algorithm on that comment. Three such algorithms are TextBlob, VADER, and Perspective API. In this paper, we audit these three algorithms to test for bias against certain racial and gender groups. More specifically, we develop a dataset of sentences with three levels of hard-coded sentiments: positive, neutral, and negative. After developing a method to replace the racial and gender identity in the sentences, we query each sentiment analysis algorithm with each combination of race and gender. Statistical tests are computed on the output to determine if there is a significant bias in how each algorithm treats the different race and gender combinations. Our results from one-way ANOVA and Tukey's HSD find that Perspective API tended to score sentences with racial or gender identifiers as more toxic, with ``white" yielding the highest toxicity scores and ``Asian" the lowest. TextBlob showed variations in sentiment scores across different identities, such as assigning more negative sentiment to sentences with ``black." VADER, however, did not exhibit significant differences in sentiment scores among identities. Statistical tests confirmed significant differences in sentiment scores among identities for Perspective API and TextBlob, but not for VADER. The study highlights the importance of understanding underlying factors contributing to disparities in sentimental analysis algorithms and suggests future research should focus on using validated datasets and manual processing of identity terms to promote transparency in algorithmic decision-making.
